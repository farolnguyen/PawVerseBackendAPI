{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üêæ PawVerse AI Try-On Demo\n",
        "\n",
        "**Virtual Product Try-On for Pets using AI**\n",
        "\n",
        "This notebook demonstrates an AI-powered try-on system for pet products using:\n",
        "- **YOLO11** for pet detection\n",
        "- **Stable Diffusion 1.5** for image generation\n",
        "- **ControlNet Canny** for pose control\n",
        "- **Streamlit** for interactive web interface\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Prerequisites\n",
        "\n",
        "**Kaggle Settings:**\n",
        "1. Enable GPU: Settings ‚Üí Accelerator ‚Üí GPU T4 x2\n",
        "2. Enable Internet: Settings ‚Üí Internet ‚Üí ON\n",
        "\n",
        "**Required Datasets:**\n",
        "1. `tryon-metadata` - Product metadata JSON\n",
        "2. `tryon-products` - Product images\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# Install required packages\n",
        "!pip install -q diffusers transformers accelerate xformers\n",
        "!pip install -q ultralytics opencv-python streamlit pyngrok\n",
        "\n",
        "print(\"‚úÖ All packages installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Step 2: Check Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üêç Python version:\", sys.version)\n",
        "print(\"üî• PyTorch version:\", torch.__version__)\n",
        "print(\"üéÆ CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"üì± GPU:\", torch.cuda.get_device_name(0))\n",
        "    print(\"üíæ GPU Memory:\", torch.cuda.get_device_properties(0).total_memory / 1024**3, \"GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected! This will be very slow.\")\n",
        "\n",
        "# Check datasets\n",
        "print(\"\\nüìÇ Checking datasets...\")\n",
        "metadata_path = Path(\"/kaggle/input/tryon-metadata/tryon_metadata.json\")\n",
        "products_path = Path(\"/kaggle/input/tryon-products/datatryon\")\n",
        "\n",
        "print(f\"  Metadata: {'‚úÖ' if metadata_path.exists() else '‚ùå'} {metadata_path}\")\n",
        "print(f\"  Products: {'‚úÖ' if products_path.exists() else '‚ùå'} {products_path}\")\n",
        "\n",
        "if products_path.exists():\n",
        "    product_files = list(products_path.glob(\"*.png\"))\n",
        "    print(f\"  Found {len(product_files)} product images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Step 3: Create Supporting Files\n",
        "\n",
        "Create the inference pipeline and Streamlit app files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile inference_pipeline.py\n",
        "\"\"\"PASTE CONTENT OF inference_pipeline.py HERE\"\"\"\n",
        "\n",
        "# NOTE: Copy the complete inference_pipeline.py content here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile tryon_app.py\n",
        "\"\"\"PASTE CONTENT OF tryon_streamlit_app.py HERE\"\"\"\n",
        "\n",
        "# NOTE: Copy the complete tryon_streamlit_app.py content here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Step 4: Test Pipeline (Optional)\n",
        "\n",
        "Test the inference pipeline before running Streamlit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from inference_pipeline import TryOnPipeline\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize pipeline (this will take 1-2 minutes)\n",
        "print(\"‚è≥ Loading models...\")\n",
        "pipeline = TryOnPipeline()\n",
        "print(\"‚úÖ Pipeline ready!\")\n",
        "\n",
        "# Test with a sample image (optional)\n",
        "# Uncomment and provide your test image\n",
        "# test_image = Image.open(\"/path/to/test/dog.jpg\")\n",
        "# result = pipeline.generate(\n",
        "#     image=test_image,\n",
        "#     product_id=\"bowl_001\",\n",
        "#     style_id=\"chibi\"\n",
        "# )\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.imshow(test_image)\n",
        "# plt.title(\"Input\")\n",
        "# plt.axis('off')\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.imshow(result['image'])\n",
        "# plt.title(\"Output\")\n",
        "# plt.axis('off')\n",
        "# plt.show()\n",
        "# print(f\"‚è±Ô∏è Processing time: {result['processing_time']:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 5: Run Streamlit App\n",
        "\n",
        "This will:\n",
        "1. Start Streamlit server on port 8501\n",
        "2. Create a public URL using ngrok\n",
        "3. Display the URL to access the app\n",
        "\n",
        "**Note:** You need to add your ngrok auth token. Get it free from https://ngrok.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup ngrok for public URL\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# IMPORTANT: Add your ngrok token here\n",
        "# Get free token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "NGROK_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"\n",
        "\n",
        "if NGROK_TOKEN != \"YOUR_NGROK_TOKEN_HERE\":\n",
        "    conf.get_default().auth_token = NGROK_TOKEN\n",
        "    print(\"‚úÖ ngrok configured\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Please add your ngrok token above!\")\n",
        "    print(\"Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# Kill any existing streamlit processes\n",
        "!pkill -f streamlit\n",
        "\n",
        "# Start streamlit in background\n",
        "!streamlit run tryon_app.py --server.port 8501 --server.headless true &\n",
        "\n",
        "# Wait for server to start\n",
        "import time\n",
        "print(\"‚è≥ Starting Streamlit server...\")\n",
        "time.sleep(10)\n",
        "print(\"‚úÖ Streamlit server started!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create public URL\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ STREAMLIT APP IS READY!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüåê Public URL: {public_url}\")\n",
        "print(f\"\\nüì± Local URL: http://localhost:8501\")\n",
        "print(\"\\nüí° Share this URL with your teacher to demo the app!\")\n",
        "print(\"\\n‚ö†Ô∏è Keep this notebook running while using the app\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Step 6: Keep Running\n",
        "\n",
        "Keep this cell running to maintain the Streamlit server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keep notebook alive\n",
        "import time\n",
        "\n",
        "print(\"üîÑ Server is running...\")\n",
        "print(\"Press ‚èπÔ∏è Stop button to shutdown\")\n",
        "print(\"\\nYou can now use the app through the URL above!\\n\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(60)\n",
        "        print(\"üíö Server still running...\", time.strftime(\"%H:%M:%S\"))\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚èπÔ∏è Shutting down...\")\n",
        "    ngrok.disconnect(public_url)\n",
        "    !pkill -f streamlit\n",
        "    print(\"‚úÖ Shutdown complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä System Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU memory usage\n",
        "if torch.cuda.is_available():\n",
        "    print(\"üìä GPU Memory Usage:\")\n",
        "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"  Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "    print(f\"  Total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üõë Cleanup (Run when done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop all services\n",
        "ngrok.kill()\n",
        "!pkill -f streamlit\n",
        "print(\"‚úÖ All services stopped\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
